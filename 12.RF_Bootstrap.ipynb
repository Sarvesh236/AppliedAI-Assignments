{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sNKZq4XrXQh"
   },
   "source": [
    "# <font color='red'><b>Bootstrap assignment</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAHap1Z3FZC-"
   },
   "source": [
    "<b>There will be some functions that start with the word \"grader\" ex: grader_sampples(), grader_30().. etc, you should not change those function definition.\n",
    "\n",
    "Every Grader function has to return True.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuxBq_bvrwh2"
   },
   "source": [
    "<font color='blue'> <b>Importing packages</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "m6ag91ijrQOs"
   },
   "outputs": [],
   "source": [
    "import numpy as np # importing numpy for numerical computation\n",
    "from sklearn.datasets import load_boston # here we are using sklearn's boston dataset\n",
    "from sklearn.metrics import mean_squared_error # importing mean_squared_error metric\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "from operator import add\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CcHOsONTt1K_"
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "x = boston.data #independent variables\n",
    "y = boston.target #target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pc1htEFYuLRj",
    "outputId": "f5b60712-98b3-4cdc-b629-3546c1e3859c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "kQle3T_wuOa3",
    "outputId": "521c7bdd-5316-48d5-c534-b61d170d2c28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n",
       "        6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02,\n",
       "        1.5300e+01, 3.9690e+02, 4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "        6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "        1.7800e+01, 3.9690e+02, 9.1400e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6rShd89t552"
   },
   "source": [
    "## <font color='red'><b>A few key points</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdgTUXTouHEd"
   },
   "source": [
    "* Remember that the datapoints used for calculating MSE score contain some datapoints that were initially used while training the base learners (the 60% sampling). This makes these datapoints partially seen (i.e. the datapoints used for calculating the MSE score are a mixture of seen and unseen data).\n",
    "Whereas, the datapoints used for calculating OOB score have only the unseen data. This makes these datapoints completely unseen and therefore appropriate for testing the model's performance on unseen data.\n",
    "\n",
    "* Given the information above, if your logic is correct, the calculated MSE score should be less than the OOB score.\n",
    "\n",
    "* The MSE score must lie between 0 and 10.\n",
    "* The OOB score must lie between 10 and 35.\n",
    "\n",
    "* The difference between the left nad right confidence-interval values must not be more than 10. Make sure this is true for both MSE and OOB confidence-interval values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2fHTdS_zpgG"
   },
   "source": [
    "# <font color='blue'> <b>Task - 1</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0yGBuryOwHz"
   },
   "source": [
    "<font color='blue'><b>Step - 1</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJXX8vf3z073"
   },
   "source": [
    "*  <font color='blue'><b>Creating samples</b></font><br>\n",
    "    <b> Randomly create 30 samples from the whole boston data points</b>\n",
    "    *  Creating each sample: Consider any random 303(60% of 506) data points from whole data set and then replicate any 203 points from the sampled points\n",
    "    \n",
    "     For better understanding of this procedure lets check this examples, assume we have 10 data points [1,2,3,4,5,6,7,8,9,10], first we take 6 data points randomly , consider we have selected [4, 5, 7, 8, 9, 3] now we will replicate 4 points from [4, 5, 7, 8, 9, 3], consder they are [5, 8, 3,7] so our final sample will be [4, 5, 7, 8, 9, 3, 5, 8, 3,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_oWoN97BhDY"
   },
   "source": [
    "*  <font color='blue'><b> Write code for generating samples</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ph_6D2SDzz7F"
   },
   "outputs": [],
   "source": [
    "def generating_samples(input_data, target_data):\n",
    "\n",
    "    '''In this function, we will write code for generating 30 samples '''\n",
    "    \n",
    "    index = []\n",
    "    rows = np.random.choice(len(input_data), size=303, replace=False) # generating 60% of points randomly without replacement\n",
    "    s = np.random.choice(rows, size=203, replace=True) # generating 40% of points randomly from rows with replacement\n",
    "    rs = np.hstack((rows,s))                    # combining them\n",
    "    x = input_data[rs]\n",
    "    \n",
    "    no_of_columns = random.randint(3, 13)      # number of columns in each sample.\n",
    "    columns = np.array(random.sample(range(0, 13), no_of_columns)) #randomy selecting the columns based on random sample generator\n",
    "    x_mod = x[:, columns]\n",
    "    len(x_mod)\n",
    "    y_mod = target_data[rs]\n",
    "    \n",
    "    return x_mod,y_mod,rows,columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MivEQFlm7iOg"
   },
   "source": [
    "<font color='cyan'> <b> Grader function - 1 </b> </fongt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AVvuhNzm7uld"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_samples(a,b,c,d):\n",
    "    length = (len(a)==506  and len(b)==506)\n",
    "    sampled = (len(a)-len(set([str(i) for i in a]))==203)\n",
    "    rows_length = (len(c)==303)\n",
    "    column_length= (len(d)>=3)\n",
    "    assert(length and sampled and rows_length and column_length)\n",
    "    return True\n",
    "a,b,c,d = generating_samples(x, y)\n",
    "grader_samples(a,b,c,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4LSsmn4Jn2_"
   },
   "source": [
    "\n",
    "* <font color='blue'><b> Create 30 samples </b></font>\n",
    "    *  Note that as a part of the Bagging when you are taking the random samples <b>make sure each of the sample will have different set of columns</b><br>\n",
    "Ex: Assume we have 10 columns[1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10] for the first sample we will select [3, 4, 5, 9, 1, 2] and for the second sample  [7, 9, 1, 4, 5, 6, 2] and so on...\n",
    "Make sure each sample will have atleast 3 feautres/columns/attributes\n",
    "\n",
    "* <font color='red'><b> Note - While selecting the random 60% datapoints from the whole data, make sure that the selected datapoints are all exclusive, repetition is not allowed. </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XXlKWjCcBvTk"
   },
   "outputs": [],
   "source": [
    "# Use generating_samples function to create 30 samples \n",
    "# store these created samples in a list\n",
    "\n",
    "list_input_data = []\n",
    "list_output_data = []\n",
    "list_selected_row = []\n",
    "list_selected_columns = []\n",
    "\n",
    "for i in range(0,30):\n",
    "\n",
    "    a,b,c,d = generating_samples(x,y)\n",
    "\n",
    "    list_input_data.append(a)\n",
    "    list_output_data.append(b)\n",
    "    list_selected_row.append(c)\n",
    "    list_selected_columns.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXUz9VFiMQkh"
   },
   "source": [
    "<font color='cyan'> <b>Grader function - 2 </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hCvIq8NuMWOC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_30(a):\n",
    "    assert(len(a)==30 and len(a[0])==506)\n",
    "    return True\n",
    "grader_30(list_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Pv-mkZkO6dh"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whaHCPB0O8qF"
   },
   "source": [
    "<font color='red'><b>Step - 2 </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xvH06HPQBdP"
   },
   "source": [
    "<font color='blue'><b>Building High Variance Models on each of the sample and finding train MSE value</b></font>\n",
    "\n",
    "*  Build a regression trees on each of 30 samples.\n",
    "*  Computed the predicted values of each data point(506 data points) in your corpus.\n",
    "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{30}\\sum_{k=1}^{30}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$\n",
    "*  Now calculate the $MSE =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRwPO_uHQjul"
   },
   "source": [
    "*  <font color='blue'><b> Write code for building regression trees</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YWQp6tRwMthq"
   },
   "outputs": [],
   "source": [
    "all_DT_models = []\n",
    "\n",
    "for i in range(0, 30):\n",
    "    model = DecisionTreeRegressor(max_depth=None)\n",
    "    model.fit(list_input_data[i], list_output_data[i])\n",
    "    all_DT_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e-UamlHRjPy"
   },
   "source": [
    "After getting predicted_y for each data point, we can use sklearns mean_squared_error to calculate the MSE between predicted_y and actual_y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnIMT7_oR312"
   },
   "source": [
    "*  <font color='blue'><b> Write code for calculating MSE</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qWhcvMRWRA9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is :  0.6018864130434789\n"
     ]
    }
   ],
   "source": [
    "all_pred_y = []\n",
    "\n",
    "for i in range(0, 30):\n",
    "    \n",
    "    data = x[:, list_selected_columns[i]]\n",
    "    pred_y = all_DT_models[i].predict(data)\n",
    "    all_pred_y.append(pred_y)\n",
    "\n",
    "all_pred_y = np.median(all_pred_y,axis=0)\n",
    "mse = mean_squared_error(y, all_pred_y)\n",
    "print(\"The MSE is : \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuclPDMnSz8F"
   },
   "source": [
    "<font color='blue'><b>Step - 3 </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW3GOcFzTqbt"
   },
   "source": [
    "*  Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{k}\\sum_{\\text{k= model which was buit on samples not included } x^{i}}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.\n",
    "*  Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBqcS03pUYSZ"
   },
   "source": [
    "*  <font color='blue'><b> Write code for calculating OOB score </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Fog_6DNdS-h_",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OOB score is :  14.512286723254022\n"
     ]
    }
   ],
   "source": [
    "all_pred_oob = []\n",
    "score = 0\n",
    "\n",
    "for i in range(0, 506):     # Total number of rows\n",
    "    oob_indices = []\n",
    "    \n",
    "    # For each of i build a sample of size 30 which should not be part of the list_selected_row[i]\n",
    "    # For eg, say for i=230 and j in the loop is 5 then list_selected_row[5] should not contain the 230th row\n",
    "    \n",
    "    for j in range(0, 30):\n",
    "        if i not in list_selected_row[j]:\n",
    "            oob_indices.append(j)\n",
    "            \n",
    "    pred_oob = []\n",
    "    \n",
    "    for index in oob_indices:\n",
    "        model_o = all_DT_models[index]\n",
    "        \n",
    "        oob_row = x[i]\n",
    "        \n",
    "        # extracting ONLY the columns that are selected during the bootstrapping\n",
    "        oob_x = [oob_row[col] for col in list_selected_columns[index]] \n",
    "        oob_x = np.array(oob_x).reshape(1, -1)\n",
    "        \n",
    "        y_pred_oob = model_o.predict(oob_x)\n",
    "        pred_oob.append(y_pred_oob)\n",
    "        \n",
    "    pred_oob = np.array(pred_oob)\n",
    "    pred_oob = np.median(pred_oob)\n",
    "    \n",
    "    all_pred_oob.append(pred_oob)\n",
    "    \n",
    "    score += ((y[i] - all_pred_oob[i] ) ** 2)\n",
    "    \n",
    "\n",
    "oob_score = score/506\n",
    "print(\"The OOB score is : \", oob_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbuiwX3OUjUI"
   },
   "source": [
    "# <font color='blue'><b>Task 2</b></font>\n",
    "\n",
    "*  <font color='blue'><b>Computing CI of OOB Score and Train MSE</b></font>\n",
    "  *   Repeat Task 1 for 35 times, and for each iteration store the Train MSE and OOB score </li>\n",
    "<li> After this we will have 35 Train MSE values and 35 OOB scores </li>\n",
    "<li> using these 35 values (assume like a sample) find the confidence intravels of MSE and OOB Score </li>\n",
    "<li> you need to report CI of MSE and CI of OOB Score </li>\n",
    "<li> Note: Refer the Central_Limit_theorem.ipynb to check how to find the confidence intravel</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ceW5-D88Uswi"
   },
   "outputs": [],
   "source": [
    "def calculate_mse_obb(x, y):\n",
    "    \n",
    "    list_input_data = []\n",
    "    list_output_data = []\n",
    "    list_selected_row = []\n",
    "    list_selected_columns = []\n",
    "\n",
    "    for i in range(0,30):\n",
    "\n",
    "        a,b,c,d = generating_samples(x,y)\n",
    "\n",
    "        list_input_data.append(a)\n",
    "        list_output_data.append(b)\n",
    "        list_selected_row.append(c)\n",
    "        list_selected_columns.append(d)\n",
    "        \n",
    "    all_DT_models = []\n",
    "\n",
    "    for i in range(0, 30):\n",
    "        model = DecisionTreeRegressor(max_depth=None)\n",
    "        model.fit(list_input_data[i], list_output_data[i])\n",
    "        all_DT_models.append(model)\n",
    "        \n",
    "    all_pred_y = []\n",
    "\n",
    "    for i in range(0, 30):\n",
    "\n",
    "        data = x[:, list_selected_columns[i]]\n",
    "        pred_y = all_DT_models[i].predict(data)\n",
    "        all_pred_y.append(pred_y)\n",
    "\n",
    "    all_pred_y = np.median(all_pred_y,axis=0)\n",
    "    mse = mean_squared_error(y, all_pred_y)       # MSE calculation\n",
    "    \n",
    "    \n",
    "    # OOB Score calculation\n",
    "    \n",
    "    all_pred_oob = []\n",
    "    score = 0\n",
    "\n",
    "    for i in range(0, 506):     # Total number of rows\n",
    "        oob_indices = []\n",
    "\n",
    "        # For each of i build a sample of size 30 which should not be part of the list_selected_row[i]\n",
    "        # For eg, say for i=230 and j in the loop is 5 then list_selected_row[5] should not contain the 230th row\n",
    "\n",
    "        for j in range(0, 30):\n",
    "            if i not in list_selected_row[j]:\n",
    "                oob_indices.append(j)\n",
    "\n",
    "        pred_oob = []\n",
    "\n",
    "        for index in oob_indices:\n",
    "            model_o = all_DT_models[index]\n",
    "\n",
    "            oob_row = x[i]\n",
    "\n",
    "            # extracting ONLY the columns that are selected during the bootstrapping\n",
    "            oob_x = [oob_row[col] for col in list_selected_columns[index]] \n",
    "            oob_x = np.array(oob_x).reshape(1, -1)\n",
    "\n",
    "            y_pred_oob = model_o.predict(oob_x)\n",
    "            pred_oob.append(y_pred_oob)\n",
    "\n",
    "        pred_oob = np.array(pred_oob)\n",
    "        pred_oob = np.median(pred_oob)\n",
    "\n",
    "        all_pred_oob.append(pred_oob)\n",
    "\n",
    "        score += ((y[i] - all_pred_oob[i] ) ** 2)\n",
    "\n",
    "\n",
    "    oob_score = score/506\n",
    "    \n",
    "    return mse,oob_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_list = []\n",
    "oob_list = []\n",
    "\n",
    "for i in range(0,35):\n",
    "    mse,oob = calculate_mse_obb(x, y)\n",
    "    mse_list.append(mse)\n",
    "    oob_list.append(oob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of 35 models : \n",
      " [0.05437747035573131, 0.037040513833992074, 0.042792292490118546, 0.050514382960035124, 0.24589856139901428, 0.17584705753184013, 0.21595479249011867, 0.1933448616600789, 0.23908116490996933, 0.030254446640316183, 0.07626736968873526, 0.14677371541501977, 0.02986660079051389, 0.0750944224857269, 0.011828063241106732, 0.1547858418071692, 0.04511363636363635, 0.036931818181818184, 0.02707509881422929, 0.1567037353845066, 0.1408975898161941, 0.009550395256917015, 0.1015374687424376, 0.10088438735177874, 0.028890118577075116, 0.07846343873517794, 0.033116506246035224, 0.026932213438735194, 0.013942138779095333, 0.04849802371541504, 1.0406077075098816, 0.15320707070707063, 0.10567087176108925, 0.16582724473462943, 0.047840909090909094]\n",
      "\n",
      "OOB score of 35 models : \n",
      " [11.57631916996048, 14.491279644268776, 14.355757164031623, 13.200539714421545, 13.533518929004776, 13.291527936534273, 12.942057556336783, 14.06044960474308, 15.118338944280367, 12.874036561264818, 15.743393869721677, 16.863376703173042, 11.496482213438739, 11.02156311041275, 14.886506841821053, 14.05547157899689, 14.623165386072765, 13.67869994408908, 12.425323616600789, 12.322736774322893, 13.310495427114144, 11.475884387351776, 14.394875562068847, 16.21000494071146, 11.475394466403156, 12.436647371931281, 11.885841037707651, 15.928790830699924, 13.85324575856858, 14.935128458498026, 18.601986880324162, 10.920287796442675, 15.313497474747484, 15.92754227439748, 14.610832784788983]\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE of 35 models : \\n\",mse_list)\n",
    "print(\"\\nOOB score of 35 models : \\n\",oob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CI for MSE :  (0.058307788771814355, 0.17834432156567812)\n",
      "CI for OOB :  (13.207911905567054, 14.440145278161618)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "con_level = 0.95           # As mentioned in Central_Limit_theorem.ipynb\n",
    "deg_of_freedom = 34        # deg_of_freedom = sample size-1\n",
    "\n",
    "# reference = https://www.adamsmith.haus/python/answers/how-to-compute-the-confidence-interval-of-a-sample-statistic-in-python\n",
    "# Confidence interval for MSE\n",
    "\n",
    "mean = np.mean(mse_list)\n",
    "std_err = scipy.stats.sem(mse_list)\n",
    "\n",
    "CI_MSE = scipy.stats.t.interval(con_level, deg_of_freedom, mean, std_err)\n",
    "print(\"CI for MSE : \",CI_MSE)\n",
    "\n",
    "# Confidence interval for OOB\n",
    "\n",
    "mean_oob = np.mean(oob_list)\n",
    "std_err_oob = scipy.stats.sem(oob_list)\n",
    "\n",
    "CI_OOB = scipy.stats.t.interval(con_level, deg_of_freedom, mean_oob, std_err_oob)\n",
    "print(\"CI for OOB : \",CI_OOB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKTnJdiBVS_e"
   },
   "source": [
    "# <font color='blue'><b>Task 3</b></font>\n",
    "\n",
    "*  <font color='blue'><b>Given a single query point predict the price of house.</b></font>\n",
    "\n",
    "Consider xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60] \n",
    "Predict the house price for this point as mentioned in the step 2 of Task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyjwEJ62V6a6"
   },
   "source": [
    "<b>Hint: </b> We created 30 models by using 30 samples in TASK-1. Here, we need send query point \"xq\"  to 30 models and perform the regression on the output generated by 30 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29hjwKlWWDfo"
   },
   "source": [
    "*  <font color='blue'><b> Write code for TASK 3 </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "i_pUlSD-VYD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House Price for query point xq is :  18.5\n",
      "House Price for query point xq1 is :  21.35\n"
     ]
    }
   ],
   "source": [
    "def predict_house_price(x_q):\n",
    "    \n",
    "    y_pred_all = []\n",
    "  \n",
    "    for i in range(0, 30):\n",
    "        model1 = all_DT_models[i]\n",
    "        \n",
    "        # Extract x for ith data point with specific number of featues from list_selected_columns\n",
    "        \n",
    "        x = [x_q[col] for col in list_selected_columns[i]]\n",
    "        x = np.array(x).reshape(1, -1)\n",
    "        y_pred = model1.predict(x)\n",
    "        y_pred_all.append(y_pred)\n",
    "\n",
    "    y_pred_median = np.median(y_pred_all)\n",
    "    \n",
    "    return y_pred_median\n",
    "\n",
    "\n",
    "xq = [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60] \n",
    "y_q = predict_house_price(xq)\n",
    "print(\"House Price for query point xq is : \",y_q)\n",
    "\n",
    "xq1 = [i*2 for i in xq]\n",
    "y_q1 = predict_house_price(xq1)\n",
    "print(\"House Price for query point xq1 is : \",y_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJHTGEZgWJjR"
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOdUi-0xWOJ9"
   },
   "source": [
    "<font color='red'><b>Write observations for task 1, task 2, task 3 indetail</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIcax45hWKT-"
   },
   "source": [
    "##### Task 1 :\n",
    "- We do row sampling & column sampling to reduce the variance and to build strong models.\n",
    "- We created <b>High Variance</b> models of `DecisionTreeRegressor` by keeping `max_depth = None`\n",
    "- OOB Score is the error on samples that were not seen during the training.\n",
    "- OOB Scoring is very useful when dataset is small and thereby when splitted into training and validation set - will result in loss of useful data that otherwise could have been used for training the models. Hence in this case, we decide to extract some of the training data as the validation set by using only those data-points that were not used for training a particular sample-set.\n",
    "- We got the MSE score between 0 & 10 and OOB score between 10 and 35 and also MSE is less than OOB score.\n",
    "\n",
    "##### Task 2 :\n",
    "- By definition we know the interpretation of a 95% confidence interval for the population mean as - If repeated random samples were taken and the 95% confidence interval was computed for each sample, 95% of the intervals would contain the population mean.\n",
    "- Therefore in our case:\n",
    "    - <b>MSE</b> - There is a 95% chance that the confidence interval of <b>(0.058307788771814355, 0.17834432156567812)</b> contains the true population mean of MSE.\n",
    "    - <b>OOB Score</b> - There is a 95% chance that the confidence interval of <b>(13.207911905567054, 14.440145278161618)</b> contains the true population mean of OOB Score.\n",
    "\n",
    "##### Task 3 :\n",
    "- Our goal through <b>Bagging</b> is to <b>reduce the variance</b> of the final model maintaining the <b>same Bias</b>\n",
    "- As we can see for <b>xq</b> we got the output as <b>18.5</b>.\n",
    "- We can see all the elements in <b>xq1</b> is 1.5 times the <b>xq</b>, since we got a low variance model the output we got is <b>21.35</b>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bootstrap_assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
